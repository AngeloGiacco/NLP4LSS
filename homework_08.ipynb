{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Causal Language Model Using PyTorch\n",
        "In notebook07 we learned how to build a causal language model (GPT) from scratch. In this homework, we will build a Causal Language model with PyTorch, which also supports back-propagation compared to the notebook implementation."
      ],
      "metadata": {
        "id": "Rxg_49Pykv4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------"
      ],
      "metadata": {
        "id": "P7zU94KM3shV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to train our GPT on this corpus"
      ],
      "metadata": {
        "id": "S9wkGm3c4exk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8fIvdRp39FS",
        "outputId": "6843d213-72af-44b4-dc60-fab263a51aa2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-16 12:26:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-04-16 12:26:59 (19.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVXpPsyg35Kn",
        "outputId": "6adcd34c-3532-4a9a-de75-9f8c5ac24da2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, character-level tokenization is applied here instead of learning a new BPE tokenizer"
      ],
      "metadata": {
        "id": "zh4FqwhvGLFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "bhh1K4sD4lxG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO1: Single-head Attention Implementation"
      ],
      "metadata": {
        "id": "pL_gJtvIHcXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # register_buffer simply adds self.tril as a non-paramater buffer in this nn.Module\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: fill in the forward pass for single-head attention\n",
        "        _,t,c = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "        # verbose names for my own understanding\n",
        "        scaled_attention = q @ k.transpose(-2,-1) * c**-0.5\n",
        "        causal_scaled_attention = scaled_attention.masked_fill(self.tril[:t, :t] == 0, float('-inf'))\n",
        "        normalised_causal_scaled_attention = F.softmax(causal_scaled_attention, dim=-1)\n",
        "        dropout_normalised_causal_scaled_attention = self.dropout(normalised_causal_scaled_attention)\n",
        "        return dropout_normalised_causal_scaled_attention @ v"
      ],
      "metadata": {
        "id": "dagStHrbGci4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head attention and feed forward layer implementations"
      ],
      "metadata": {
        "id": "O9PDdtZCIvAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "VwoaebhhIoQw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO2: Transformer Block Implementation"
      ],
      "metadata": {
        "id": "CBSMLdl5I-GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: fill in the forward pass\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        # Hint: don't forget the residual connections\n",
        "        return x"
      ],
      "metadata": {
        "id": "ggI9pHGSI-aS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language Model Implementation."
      ],
      "metadata": {
        "id": "uiSY9oQLJnzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "LNMoB0I9JWnt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "model = LanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M99dsYCOMUfx",
        "outputId": "c88634e6-b8a3-4bb2-e5e5-ab1b49f5153f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.3777, val loss 4.3841\n",
            "step 100: train loss 2.6692, val loss 2.6921\n",
            "step 200: train loss 2.5086, val loss 2.5197\n",
            "step 300: train loss 2.4133, val loss 2.4310\n",
            "step 400: train loss 2.3655, val loss 2.3799\n",
            "step 500: train loss 2.2960, val loss 2.3238\n",
            "step 600: train loss 2.2490, val loss 2.2725\n",
            "step 700: train loss 2.1979, val loss 2.2205\n",
            "step 800: train loss 2.1682, val loss 2.2012\n",
            "step 900: train loss 2.1183, val loss 2.1532\n",
            "step 1000: train loss 2.0870, val loss 2.1227\n",
            "step 1100: train loss 2.0516, val loss 2.0994\n",
            "step 1200: train loss 2.0114, val loss 2.0872\n",
            "step 1300: train loss 2.0029, val loss 2.0622\n",
            "step 1400: train loss 1.9754, val loss 2.0327\n",
            "step 1500: train loss 1.9545, val loss 2.0235\n",
            "step 1600: train loss 1.9321, val loss 2.0108\n",
            "step 1700: train loss 1.9130, val loss 1.9940\n",
            "step 1800: train loss 1.8858, val loss 1.9841\n",
            "step 1900: train loss 1.8772, val loss 1.9691\n",
            "step 2000: train loss 1.8507, val loss 1.9828\n",
            "step 2100: train loss 1.8570, val loss 1.9645\n",
            "step 2200: train loss 1.8319, val loss 1.9471\n",
            "step 2300: train loss 1.8209, val loss 1.9387\n",
            "step 2400: train loss 1.8150, val loss 1.9164\n",
            "step 2500: train loss 1.7960, val loss 1.9194\n",
            "step 2600: train loss 1.7893, val loss 1.9178\n",
            "step 2700: train loss 1.7758, val loss 1.9132\n",
            "step 2800: train loss 1.7736, val loss 1.9147\n",
            "step 2900: train loss 1.7687, val loss 1.8940\n",
            "step 2999: train loss 1.7575, val loss 1.8950\n",
            "\n",
            "To swervije do think a deserance do worth, now and reand, the,\n",
            "This weap it the rinitibre thithes.\n",
            "\n",
            "So MARERCE:\n",
            "Which, till a\n",
            "Tove do the plake man\n",
            "By woe hoursel for sout-for hee.\n",
            "\n",
            "KING RICHARD II:\n",
            "I larry a dome is so are spore\n",
            "me it that niest sove to the lend?\n",
            "\n",
            "AUFIAREO:\n",
            "I'll the siret\n",
            "The willsting thee, outheryar. what hand such to more oner conty many, into take do the in maled flore.\n",
            "\n",
            "GROLIO:\n",
            "As 'Tis that world it havoltely Moldhen to hoursely to talk'd with high.'\n",
            "Booke Serve hands your lime his upjas of sweed--I puse\n",
            "But it of humbe theme oldanges fand:\n",
            "Exter, for a it if opeous: our fay,--tell dewearet carmes and\n",
            "deah son make my unse age upon?\n",
            "Of see leave will be not\n",
            "riest hourse-nauke hather's line,\n",
            "Well a biese in this deed.\n",
            "\n",
            "HENRY Seen on\n",
            "Whather of thinks o'sing, tore geneds for housorten a master,\n",
            "O I some me suide now ploors! coursely tran:\n",
            "When, and tresten in the fore\n",
            "The and he that thinks to lear, thou comme now;\n",
            "An me had, is the was chimploseign and\n",
            "To breellies; by thee hourtand but in healt! whas that his nonpue horths;\n",
            "But that thou bond the wailigh's to sprokeds;\n",
            "Try'ts love bound in drape best I\n",
            "Fee more the his love dothel\n",
            "For onfindy man, more is thounrie: ady: andsel, it world I my say wend in your lase;\n",
            "And nothe lees forel a was to goos thy Gon:\n",
            "Where unech reyed, may was the pead.\n",
            "\n",
            "CAPULET:\n",
            "And spir, and than a lord's feep; Vill.\n",
            "\n",
            "AUTOLO:\n",
            "I that cousel in to untoge, a wosten,\n",
            "Thy ance maw--poor it shuell.\n",
            "\n",
            "That Servious and the Mwall my lord; will nor bear his he ours\n",
            "To to Vallond hew the peents leigne\n",
            "And was, say, muke in woe? be?\n",
            "\n",
            "BRUKESSEY: it her?\n",
            "\n",
            "LEON EDIA:\n",
            "When Claul own quilloved, let, I ware shall fall you nobse a more a fienderve in here respies? I warn 'lear; my any howell,\n",
            "The will mine, what, will why afeal not would frieblock deale on a ploand:\n",
            "Yelver drue in death the roudin'd?\n",
            "Nor, your worsel. Mord's. He tue rue, our wall me,\n",
            "And not leven of I thinse'y soll;\n",
            "Fraz; for hich not our.\n",
            "\n",
            "KING RICHALUS:\n",
            "Julines fathy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO3: BERTopic Practice\n",
        "Use BERTopic to obtain top-5 topics in AG's news dataset"
      ],
      "metadata": {
        "id": "IOOFXT_XjkGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS9MumB7lIH7",
        "outputId": "c53cabdd-b58d-4dd7-8462-86b045dc29a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-16 12:45:59--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29470338 (28M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]  28.10M   146MB/s    in 0.2s    \n",
            "\n",
            "2024-04-16 12:45:59 (146 MB/s) - ‘train.csv’ saved [29470338/29470338]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"train.csv\", header=None)\n",
        "\n",
        "# Analyze the topic distribution of first 5000 news leading sentences\n",
        "docs = df.iloc[:, 2].tolist()\n",
        "docs = docs[:5000]"
      ],
      "metadata": {
        "id": "GE0krBjqlkd2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zTUQt1A9O6o",
        "outputId": "cb8755ce-76de-4cdc-a39e-564f5f4920b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.25.2)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.2)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
            "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.11.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039299 sha256=dc191a9f9a7219ceeea71c477dfc827a635e82b6e8f3db4e3aa2efef37f8437d\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, cython, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.10\n",
            "    Uninstalling Cython-3.0.10:\n",
            "      Successfully uninstalled Cython-3.0.10\n",
            "Successfully installed bertopic-0.16.0 cython-0.29.37 hdbscan-0.8.33 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pynndescent-0.5.12 sentence-transformers-2.6.1 umap-learn-0.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: conduct topic modeling following the steps of the last section in https://github.com/elliottash/lm_lss_2024/blob/main/notebooks/08_LLMs.ipynb\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
        "model = BERTopic(verbose=True)\n",
        "topics, probabilities = model.fit_transform(docs)"
      ],
      "metadata": {
        "id": "lHrGksbZll2J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540,
          "referenced_widgets": [
            "ed4bd5dc98904608991616f2a1ceaf1d",
            "6c6e53eaaf80482d88142ad0d3d67525",
            "b986aaafc9cc469aa613d58b7fa7ee07",
            "caf97d678b414cce894c34b486cbda49",
            "e280453727d64b29898379266c53871a",
            "075aa1fdc51a470a823625e400e99dca",
            "399bc76e063b4ac79ec0a83bbadee96f",
            "fc31c62d9d4945eeb9fd4cbc34db1026",
            "addcaa60709e41bab29df09c3d0e230e",
            "f18cb15dbf62401f83c322041b78a61b",
            "2b10ec2b95174ceb8ab42b4a7900993d",
            "7bf56e656e7240049f45e88533e863b9",
            "b16bb33524ce4bd0ba673060c01d7f55",
            "81960ceae16a4ba98e3f84e8f4f5c501",
            "e9b5ac621fef42d2941fba60276a7781",
            "cd41c4b6315b45878f0e1c3d2541646d",
            "dec86fd9e21043c5b396b45ef0a24bc0",
            "4db1837c6985468a94ca29f50c62dcf5",
            "32bc8b51901d424c8d33243af8b95066",
            "8ad91a30899c44579faef3744fd51dd5",
            "6b396103f13e40299a618a4168584aec",
            "3b1da7234132428da223093e82cbac1c",
            "5bcaa4e134004cebbdebcec5e050e274",
            "4e5bedd8dc934535b26025d514d30f78",
            "933ffad2a47e415f8c623e7aef8d113b",
            "44c7f796fa2a4c6893abd0e95de4dee2",
            "ee40d7755a874dee8cd8cb96da12471c",
            "bdbc1718378f4c09b6f9421d64af14c1",
            "95103bd1e8b94da889f0b271aef04152",
            "17652992984248f1a26e55bddc39aa84",
            "e11db349bf314ec89e08012c3ca424a3",
            "cbd08f2ad2a64d1ab6f034f2908c680f",
            "0c71432882934e9f8267c45e24cb98c5",
            "c9dce71fe2e4442dbbf984c118c92ea9",
            "1222b5a0ff1147718ce4ba215bb658bc",
            "ffaa043f05034d9fb3ec80b289a35805",
            "05fd75dc6b074e1780cdcf055d233342",
            "f297630aba64430888360a7f8eb70666",
            "a183ad42385d4417b859315d50b98d2c",
            "68ad5ac7ae474a748d3f013a686f82e7",
            "a5dc0fce770543d88e24301a699ef23c",
            "58ff2a8d155a41d09937db5b34f4fd76",
            "4e9ee597cb3049ddb846b9ac9d6560d9",
            "e0145cb09ed04a5fa608b389e729c48c",
            "bee857a010d446f0af8cbfc9639fe573",
            "692896b0de3e45ee9874f3204240dc7f",
            "70f4f998e3b24a9198a3ab4cf0426f3c",
            "3e6b00bb79394458b492103249e60d34",
            "988d9abd29d14209b9b694128d3618df",
            "5e368ddfa91146c4837883828476b71f",
            "8b6ab52e42e14b50874b532d131bd801",
            "ae21fdcd3d1142959854774a3d817be9",
            "8b483ceaa38442f1adb8e45baac365c8",
            "39f3d5ded53b4dd187401c7c4f186437",
            "730e7264a9d7448b952752842cd2f3d6",
            "00e660f7a1124c9693c654cad76a9a0b",
            "609f1a10dea848f19618c8c3a01a3997",
            "d06b07a1a99343649170f51241c2380d",
            "645fc54ed8be472bb1458a5898171824",
            "a1d3c1bbef174a7badf623321275b69c",
            "526fdff4d78a40bdb2e9aca3e5a3d2e0",
            "bedc0665d85f4dcab0452b5441b96840",
            "dcce842c742749be83b210c16e03a555",
            "569f262955474f448328cf5a7dab4bcf",
            "b7151d41785a4e34a844cd0a26f6e74b",
            "df8cfb053d954b39be7f2b3531f00caf",
            "779570035b2f435d99716df233465e81",
            "8f8abf382c1b492eb5ca17e730520988",
            "b6e66d8dda2146b9866f553ced015b15",
            "b490fd6ef677478c9f123cc282face83",
            "1beaa42295be43839ce4f3a170ca65a9",
            "865a33cc9c754e6cabd9059f463c9155",
            "91e7df59023a44cb9f6e6e6765bc3669",
            "befcc009144c4c5e87ec80d80fd31536",
            "7dfcd0fe34f34e3d8d1f6e74c1b1f536",
            "83b7ab261d7143aeb758f7e7f0f38204",
            "ff26c328627f467dba6c9adcbd2778a1",
            "1331b6a3f91840709df334d1ca1493bc",
            "449434b0b553462ca202ca71588d9bc0",
            "65000e3a07574f5caa9fa0370862dcf8",
            "786f63a1de204a55bfd6c48cc74626cb",
            "c01c7d86b66b4c0aac5203964dabf456",
            "83ef0be66a9b4e8887707ad3bd49ddce",
            "2866016a5f774a1ca41c0c142e42f0aa",
            "4c584f295aaa43deb56b4fbb726076d2",
            "7eefdf0de5cc40b5afd77044fee24984",
            "44d7f73c444747518361b2c499e933be",
            "eced914df1e84981b108a1f52a3cc1a5",
            "edb288a5c4f44b0cabe973ed983d19ac",
            "3f0d4ec6694c495d937b2e97b6edb02d",
            "0ee6231087bc460eba415ba40b5cdf7b",
            "a401ff3102c342f4a4c8479b88a1b914",
            "0eb63510c5484792afc5ea6eea1bb426",
            "62188f6f0bf542af9fdd2aedf14f403a",
            "e1b1e529632843d9b3f5e9822059633b",
            "5b83d32dd4144c03bae7b97aae639ba0",
            "f672197086c74f16be2c583842f77387",
            "07221d8adb124bea907c8e9c28c32f5c",
            "174b94d34237484281fa3610b1b386ce",
            "d4ddc7d5fd794136ac2e1ead8fa3e988",
            "a0a476fffde641d1be820fdbee88fbfc",
            "02fda855e13a47e2a12d75d6d0ca77c0",
            "260d3d844a42415cba583fd6c418ea4c",
            "deeeace9262c4d28ba4260ed7c6cbdc5",
            "8e1839834add4455951350e86a391fc2",
            "b1a03e38369d4291b6c28758d1f029c0",
            "76fa36c06c3c4a619e46e1ca73bed7cc",
            "12314b6ab73345c8b8472958d781a257",
            "8c467e5b0330462ba805d1a3eae6b0c1",
            "9718e3532070411a9d54eb7ab5e3c266",
            "f3d0fb83f3c341308e4a02f349f490ed",
            "0355ad28026e4a3fac46c093dd34ed62",
            "025799e72763468da3b4a607c6e522e2",
            "b4a17a2ed1dc47cc9fb619cedc42d70f",
            "6d90528e051f4afc9932b313e8513308",
            "61266f6a99e64a8baaea81ba859db51c",
            "825982ab59d1433b84e5d317be9c3a4c",
            "f52b06807a1f427f974e7eaaa18d55c7",
            "7f818f0ecabc412e820b1b3222c51202",
            "a62cc2d54141476eaf4d1cd7254cafa6",
            "d2437a68999b41faa408635bb6280e7d",
            "750f843fa73743c99fc1dd239e25e008",
            "17665bf1f8874bb4a660f23c75c003a1",
            "10176a2bfea94e3fbe0f52dac560985e",
            "70dda25bb4854b98ae1c6416ea26633e",
            "61e06107bad047dd94580ae1e250e663",
            "20ab1ff6a31943ffb741adf8acfb2d64",
            "802931332bb74bff991e672f5d275fc2",
            "3e2932c3088c49e2b8a5f7872c55363c",
            "219967c809734b789ec3646acc98b2b5",
            "21d83e6722fd468b834a2d98db8d2e72",
            "1884f8a7f4e546f48a77ec1415438bdf"
          ]
        },
        "outputId": "cb11853d-2e06-4ae6-c092-98ff06931055"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-16 12:54:40,983 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed4bd5dc98904608991616f2a1ceaf1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bf56e656e7240049f45e88533e863b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bcaa4e134004cebbdebcec5e050e274"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9dce71fe2e4442dbbf984c118c92ea9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bee857a010d446f0af8cbfc9639fe573"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00e660f7a1124c9693c654cad76a9a0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "779570035b2f435d99716df233465e81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1331b6a3f91840709df334d1ca1493bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edb288a5c4f44b0cabe973ed983d19ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4ddc7d5fd794136ac2e1ead8fa3e988"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3d0fb83f3c341308e4a02f349f490ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "750f843fa73743c99fc1dd239e25e008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-16 12:57:29,550 - BERTopic - Embedding - Completed ✓\n",
            "2024-04-16 12:57:29,552 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-04-16 12:58:11,189 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-04-16 12:58:11,195 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-04-16 12:58:11,541 - BERTopic - Cluster - Completed ✓\n",
            "2024-04-16 12:58:11,561 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-04-16 12:58:12,367 - BERTopic - Representation - Completed ✓\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(1)"
      ],
      "metadata": {
        "id": "Wb0omqPv_LAO",
        "outputId": "449ad3a0-f4e4-4335-9b01-e633544319bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('google', 0.04650215348748798),\n",
              " ('offering', 0.03183425145505234),\n",
              " ('initial', 0.031487460845556786),\n",
              " ('public', 0.03146234675973264),\n",
              " ('search', 0.02611388341016574),\n",
              " ('ipo', 0.023716588435029284),\n",
              " ('stock', 0.0226177851707973),\n",
              " ('inc', 0.019270057164044913),\n",
              " ('shares', 0.018298428001723),\n",
              " ('price', 0.018191550957310326)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(0)"
      ],
      "metadata": {
        "id": "twarifgm_OHR",
        "outputId": "effc464c-8c2b-4b3e-eda9-5e07e30b1bdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('night', 0.015602287853145663),\n",
              " ('season', 0.01372072610978267),\n",
              " ('his', 0.013589219578398409),\n",
              " ('ap', 0.012807777115133607),\n",
              " ('league', 0.012177341385398605),\n",
              " ('inning', 0.011997679657857587),\n",
              " ('sox', 0.011087085406653532),\n",
              " ('the', 0.010214330158058482),\n",
              " ('baseball', 0.009775562291574582),\n",
              " ('win', 0.009652375596236122)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(2)"
      ],
      "metadata": {
        "id": "maCb9rln_PE6",
        "outputId": "5c0b51c5-1ee9-4de1-8959-65bddf07d92a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('najaf', 0.07284794434436874),\n",
              " ('cleric', 0.0479069295839937),\n",
              " ('shiite', 0.04393130336889755),\n",
              " ('iraqi', 0.041159032596604875),\n",
              " ('alsadr', 0.040522142641702966),\n",
              " ('radical', 0.04037810300681909),\n",
              " ('iraq', 0.03547497127242664),\n",
              " ('shrine', 0.032482663768332545),\n",
              " ('holy', 0.03176800176998323),\n",
              " ('baghdad', 0.02645435140681235)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert"
      ],
      "metadata": {
        "id": "Hu3Ah31eLPWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
